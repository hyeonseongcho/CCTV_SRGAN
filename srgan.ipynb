{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from math import log10\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.utils as utils\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageFile\n",
    "\n",
    "# import pytorch_ssim\n",
    "import data_utils\n",
    "from data_utils import CustumDataset, display_transform\n",
    "from loss import GeneratorLoss\n",
    "from model import Generator, Discriminator\n",
    "\n",
    "high_res = 128\n",
    "upscale_factor = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = CustumDataset('data/train/images', high_res, upscale_factor)\n",
    "val_set = CustumDataset('data/valid/images', high_res, upscale_factor)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(dataset=val_set, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "netG = Generator(upscale_factor).cuda()\n",
    "netD = Discriminator().cuda()\n",
    "\n",
    "generator_criterion = GeneratorLoss().cuda()\n",
    "\n",
    "optimizerG = optim.Adam(netG.parameters())\n",
    "optimizerD = optim.Adam(netD.parameters())\n",
    "\n",
    "results = {'d_loss': [], 'g_loss': [], 'd_score': [], 'g_score': [], 'psnr': [], 'ssim': []}\n",
    "\n",
    "upsample_nearest = torch.nn.Upsample(scale_factor=2, mode='nearest').cuda()\n",
    "upsample_bilinear = torch.nn.Upsample(scale_factor=2, mode='bilinear').cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    train_bar = tqdm(train_loader)\n",
    "    running_results = {'batch_sizes': 0, 'd_loss': 0, 'g_loss': 0, 'd_score': 0, 'g_score': 0}\n",
    "\n",
    "    netG.train()\n",
    "    netD.train()\n",
    "    for data, target in train_bar:\n",
    "        g_update_first = True\n",
    "        batch_size = data.size(0)\n",
    "        running_results['batch_sizes'] += batch_size\n",
    "\n",
    "        real_img = Variable(target).cuda()\n",
    "        z = Variable(data).cuda()\n",
    "        fake_img = netG(z)\n",
    "\n",
    "        netD.zero_grad()\n",
    "        real_out = netD(real_img).mean()\n",
    "        fake_out = netD(fake_img).mean()\n",
    "        d_loss = 1 - real_out + fake_out\n",
    "        d_loss.backward(retain_graph=True)\n",
    "        optimizerD.step()\n",
    "\n",
    "\n",
    "        netG.zero_grad()\n",
    "        fake_img = netG(z)\n",
    "        fake_out = netD(fake_img).mean()\n",
    "        g_loss = generator_criterion(fake_out, fake_img, real_img)\n",
    "        g_loss.backward()\n",
    "        fake_img = netG(z)\n",
    "        fake_out = netD(fake_img).mean()\n",
    "        optimizerG.step()\n",
    "\n",
    "        # loss for current batch before optimization \n",
    "        running_results['g_loss'] += g_loss.item() * batch_size\n",
    "        running_results['d_loss'] += d_loss.item() * batch_size\n",
    "        running_results['d_score'] += real_out.item() * batch_size\n",
    "        running_results['g_score'] += fake_out.item() * batch_size\n",
    "\n",
    "        train_bar.set_description(desc='[%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f' % (\n",
    "                epoch, num_epochs, running_results['d_loss'] / running_results['batch_sizes'],\n",
    "                running_results['g_loss'] / running_results['batch_sizes'],\n",
    "                running_results['d_score'] / running_results['batch_sizes'],\n",
    "                running_results['g_score'] / running_results['batch_sizes']))\n",
    "                \n",
    "    \n",
    "    \n",
    "    netG.eval()\n",
    "    out_path = 'training_results/SRF_' + str(upscale_factor) + '/'\n",
    "    if not os.path.exists(out_path):\n",
    "            os.makedirs(out_path)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        val_bar = tqdm(val_loader)\n",
    "        valing_results = {'mse': 0, 'ssims': 0, 'psnr': 0, 'ssim': 0, 'batch_sizes': 0}\n",
    "        val_images = []\n",
    "        # for val_lr, val_hr_restore, val_hr in val_bar:\n",
    "        for val_lr, val_hr in val_bar:\n",
    "            batch_size = val_lr.size(0)\n",
    "            valing_results['batch_sizes'] += batch_size\n",
    "            lr = val_lr\n",
    "            hr = val_hr\n",
    "            if torch.cuda.is_available():\n",
    "                lr = lr.cuda()\n",
    "                hr = hr.cuda()\n",
    "            sr = netG(lr)\n",
    "\n",
    "            lr_upsample = upsample_nearest(lr)\n",
    "            lr_bilinear = upsample_bilinear(lr)\n",
    "    \n",
    "            batch_mse = ((sr - hr) ** 2).data.mean()\n",
    "            valing_results['mse'] += batch_mse * batch_size\n",
    "            #batch_ssim = pytorch_ssim.ssim(sr, hr).item()\n",
    "            #valing_results['ssims'] += batch_ssim * batch_size\n",
    "            valing_results['psnr'] = 10 * log10((hr.max()**2) / (valing_results['mse'] / valing_results['batch_sizes']))\n",
    "            valing_results['ssim'] = valing_results['ssims'] / valing_results['batch_sizes']\n",
    "            val_bar.set_description(\n",
    "                desc='[converting LR images to SR images] PSNR: %.4f dB SSIM: %.4f' % (\n",
    "                    valing_results['psnr'], valing_results['ssim']))\n",
    "    \n",
    "            val_images.extend(\n",
    "                [display_transform()(lr_upsample.data.cpu().squeeze(0)), display_transform()(lr_bilinear.data.cpu().squeeze(0)), display_transform()(sr.data.cpu().squeeze(0)), display_transform()(hr.data.cpu().squeeze(0))])\n",
    "\n",
    "            '''val_images.extend(\n",
    "                [display_transform()(val_hr_restore.squeeze(0)), display_transform()(hr.data.cpu().squeeze(0)),\n",
    "                    display_transform()(sr.data.cpu().squeeze(0))])'''\n",
    "        val_images = torch.stack(val_images)\n",
    "        val_images = torch.chunk(val_images, val_images.size(0) // 32)\n",
    "        val_save_bar = tqdm(val_images, desc='[saving training results]')\n",
    "        index = 1\n",
    "        for image in val_save_bar:\n",
    "            image = utils.make_grid(image, nrow=8, padding=5)\n",
    "            utils.save_image(image, out_path + 'epoch_%d_index_%d.png' % (epoch, index), padding=5)\n",
    "            index += 1\n",
    "\n",
    "    # save loss\\scores\\psnr\\ssim\n",
    "    results['d_loss'].append(running_results['d_loss'] / running_results['batch_sizes'])\n",
    "    results['g_loss'].append(running_results['g_loss'] / running_results['batch_sizes'])\n",
    "    results['d_score'].append(running_results['d_score'] / running_results['batch_sizes'])\n",
    "    results['g_score'].append(running_results['g_score'] / running_results['batch_sizes'])\n",
    "    results['psnr'].append(valing_results['psnr'])\n",
    "    results['ssim'].append(valing_results['ssim'])\n",
    "\n",
    "    if epoch % 10 == 0 and epoch != 0:\n",
    "        out_path = 'statistics/'\n",
    "        data_frame = pd.DataFrame(\n",
    "            data={'Loss_D': results['d_loss'], 'Loss_G': results['g_loss'], 'Score_D': results['d_score'],\n",
    "                    'Score_G': results['g_score'], 'PSNR': results['psnr'], 'SSIM': results['ssim']},\n",
    "            index=range(1, epoch + 1))\n",
    "        data_frame.to_csv(out_path + f'./res{high_res}_uf{upscale_factor}.csv', index_label='Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'epoch': epoch,\n",
    "            'G_model_state_dict': netG.state_dict(),\n",
    "            'G_optimizer_state_dict': optimizerG.state_dict(),\n",
    "            'G_loss': g_loss,\n",
    "            'D_model_state_dict': netD.state_dict(),\n",
    "            'D_optimizer_state_dict': optimizerD.state_dict(),\n",
    "            'D_loss': d_loss\n",
    "            }, f'./res{high_res}_uf{upscale_factor}.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(f'./res{high_res}_uf{upscale_factor}.tar')\n",
    "netG.load_state_dict(checkpoint['G_model_state_dict'])\n",
    "netD.load_state_dict(checkpoint['D_model_state_dict'])\n",
    "optimizerG.load_state_dict(checkpoint['G_optimizer_state_dict'])\n",
    "optimizerD.load_state_dict(checkpoint['D_optimizer_state_dict'])\n",
    "\n",
    "g_loss = checkpoint['G_loss']\n",
    "d_loss = checkpoint['D_loss']\n",
    "\n",
    "netG.train()\n",
    "netD.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('CHO')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "75e71d7c28e93eec18ce6910883a2a9a22f0497d07a96546cfcbe0a235c3f893"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
